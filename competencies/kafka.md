# Competency - Kafka

Kafka combines three key capabilities so you can implement your use cases for event streaming end-to-end with a single
battle-tested solution:

1. To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.
2. To store streams of events durably and reliably for as long as you want.
3. To process streams of events as they occur or retrospectively.

And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. 
Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. 
You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.

_Source: https://kafka.apache.org/_

## How do you prove it?

* You know what Kafka is and when it can be used.

* You know the advantages and disadvantages of using Kafka.

* You know what consumer group id is used for and how it influences message delivery.

* You understand implications of using keys in messages.

* You understand what request-reply is and know how/when to use it.

* You understand how kafka authentication works and how to configure it.

* You can use at least one Kafka client.

* Set up a Kafka broker.

* Create a topic and two partitions for it.

* Produce a message to one of the created partitions.

* List all topics and read messages in one of them.

* Build a kafka queue and add a leader and a follower, show that when you kill the leader, the follower takes over.

## How do you improve it?

Read the official documentation: https://kafka.apache.org/documentation/

Do a tutorial: https://data-flair.training/blogs/apache-kafka-tutorial/

Know what to avoid: https://blog.softwaremill.com/7-mistakes-when-using-apache-kafka-44358cd9cd6

Know how to optimize your deployment: https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment